{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b3da55",
   "metadata": {},
   "source": [
    "# Chapter 2: Introduction to Gymnasium\n",
    "\n",
    "## What is Gymnasium?\n",
    "\n",
    "[Gymnasium](https://github.com/Farama-Foundation/Gymnasium) is the modern successor to OpenAI's Gym library, which became the de facto standard for reinforcement learning environments when it was first released in 2017. In 2021, development moved to the Farama Foundation, creating Gymnasium as a drop-in replacement that maintains the same API while providing ongoing support and improvements.\n",
    "\n",
    "The primary purpose of Gymnasium is to provide a unified interface for reinforcement learning experiments through a rich collection of environments. This standardization allows researchers and practitioners to write code that works across different RL scenarios, from simple grid worlds to complex robotic simulations.\n",
    "\n",
    "## Core Components\n",
    "\n",
    "Every Gymnasium environment is built around three fundamental concepts:\n",
    "\n",
    "### 1. Action Space\n",
    "The **action space** defines what actions an agent can take in the environment:\n",
    "\n",
    "- **Discrete actions**: A finite set of mutually exclusive choices (e.g., moving left, right, up, or down in a grid)\n",
    "- **Continuous actions**: Actions with continuous values within specified bounds (e.g., steering wheel angle from -720° to +720°)\n",
    "- **Combined actions**: Multiple actions that can be executed simultaneously\n",
    "\n",
    "### 2. Observation Space  \n",
    "The **observation space** specifies what information the environment provides to the agent at each timestep. Observations can range from simple numerical values to complex multi-dimensional data like images from multiple cameras.\n",
    "\n",
    "### 3. Environment Interface\n",
    "The core environment class (`Env`) provides two essential methods:\n",
    "\n",
    "- **`reset()`**: Initializes the environment to its starting state and returns the first observation\n",
    "- **`step(action)`**: Executes an action and returns:\n",
    "  - Next observation\n",
    "  - Reward earned\n",
    "  - Episode termination flag (`done`)\n",
    "  - Episode truncation flag (`truncated`) \n",
    "  - Additional environment information (`info`)\n",
    "\n",
    "## Common Space Types\n",
    "\n",
    "Gymnasium uses `Space` classes to formally define action and observation spaces:\n",
    "\n",
    "- **`Discrete(n)`**: Represents n mutually exclusive options (0 to n-1)\n",
    "- **`Box(low, high, shape)`**: Represents continuous values within bounds, with specified dimensions\n",
    "- **`Tuple`**: Combines multiple spaces for complex scenarios\n",
    "\n",
    "### Space Class Hierarchy\n",
    "\n",
    "The following diagram shows the inheritance structure of Gymnasium's Space classes:\n",
    "\n",
    "```mermaid\n",
    "classDiagram\n",
    "    class Space {\n",
    "        +shape: Tuple[int, ...]\n",
    "        +sample()\n",
    "        +contains(x)\n",
    "        +seed()\n",
    "    }\n",
    "    \n",
    "    class Box {\n",
    "        +low: float\n",
    "        +high: float\n",
    "    }\n",
    "    \n",
    "    class Discrete {\n",
    "        +n: int\n",
    "    }\n",
    "    \n",
    "    class Tuple {\n",
    "        +spaces: Tuple[Space, ...]\n",
    "    }\n",
    "    \n",
    "    Space <|-- Box\n",
    "    Space <|-- Discrete\n",
    "    Space <|-- Tuple\n",
    "```\n",
    "\n",
    "**Figure 2.1**: The hierarchy of the Space class in Gymnasium. The abstract Space class provides the foundation with core methods like `sample()`, `contains()`, and `seed()`, while its subclasses (Box, Discrete, and Tuple) implement specific types of action and observation spaces.\n",
    "\n",
    "## Basic Workflow\n",
    "\n",
    "The typical interaction pattern with a Gymnasium environment follows this loop:\n",
    "\n",
    "1. Create and initialize the environment\n",
    "2. Call `reset()` to get the initial observation\n",
    "3. In a loop:\n",
    "   - Choose an action based on the current observation\n",
    "   - Call `step(action)` to execute the action\n",
    "   - Receive new observation, reward, and termination flags\n",
    "   - Continue until episode ends (`done` or `truncated` is True)\n",
    "4. Reset and repeat for new episodes\n",
    "\n",
    "This standardized interface makes it easy to test different algorithms across various environments and ensures reproducible research in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72876a6",
   "metadata": {},
   "source": [
    "# Cartpole example\n",
    "\n",
    "The CartPole environment is a classic control problem where the goal is to balance a pole attached to a movable cart. Let's examine its action and observation spaces to understand how an agent interacts with this environment.\n",
    "\n",
    "### The CartPole Task\n",
    "\n",
    "CartPole involves controlling a platform with a pole (stick) attached to its center. The pole tends to fall left or right due to gravity, and the agent must move the platform horizontally to keep the pole balanced. The episode continues until the pole falls beyond a certain angle or the cart moves too far from the center.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The CartPole environment has a **discrete action space** with only two possible actions:\n",
    "\n",
    "- **Action 0**: Push the cart to the left\n",
    "- **Action 1**: Push the cart to the right\n",
    "\n",
    "This is represented as `Discrete(2)`, meaning the agent can choose from 2 mutually exclusive actions at each timestep.\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation space is a **Box** space containing 4 continuous values that describe the current state of the system:\n",
    "\n",
    "1. **Cart Position** (`x`): The horizontal position of the cart\n",
    "   - Range: [-4.8, 4.8] (approximate)\n",
    "   - Units: meters from center\n",
    "\n",
    "2. **Cart Velocity** (`ẋ`): The horizontal velocity of the cart  \n",
    "   - Range: [-∞, ∞]\n",
    "   - Units: meters per second\n",
    "\n",
    "3. **Pole Angle** (`θ`): The angle of the pole from vertical\n",
    "   - Range: [-0.418, 0.418] radians (approximately ±24°)\n",
    "   - Positive values mean the pole is leaning right\n",
    "\n",
    "4. **Pole Angular Velocity** (`θ̇`): The rate of change of the pole angle\n",
    "   - Range: [-∞, ∞] \n",
    "   - Units: radians per second\n",
    "\n",
    "### Reward Structure\n",
    "\n",
    "- **Reward**: +1 for every timestep the pole remains upright\n",
    "- **Episode Termination**: When the pole angle exceeds ±12° or cart position exceeds ±2.4 units\n",
    "- **Goal**: Maximize cumulative reward by keeping the pole balanced as long as possible\n",
    "\n",
    "The challenge for reinforcement learning is to learn the optimal balancing strategy using only these numerical observations and the reward signal, without explicit knowledge of the physics involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be86dd",
   "metadata": {},
   "source": [
    "## Random policy (matplotlib viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0428dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 14 steps, total reward 14.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# Initialize the environment with the render mode set to rgb_array (which enables rendering).\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment to get the initial state.\n",
    "observation, info = env.reset()\n",
    "\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "for _ in range(100):\n",
    "    # Sample a random action from the action space.\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render and display the image\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # Update the total reward and steps.\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode done in {total_steps} steps, total reward {total_reward:.2f}\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d897d",
   "metadata": {},
   "source": [
    "## Random policy (gym-notebook-wrapper)\n",
    "\n",
    "[gym-notebook-wrapper API documentation](https://pypi.org/project/gym-notebook-wrapper/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gnwrapper\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"))\n",
    "obs, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()  # Stores the frame for animation\n",
    "    obs = next_obs\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.display()  # Displays the looping animation\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866df7d",
   "metadata": {},
   "source": [
    "# Environment Wrappers\n",
    "\n",
    "Gymnasium's wrapper system provides a powerful way to extend and modify environment functionality without changing the underlying environment code. Wrappers allow you to:\n",
    "\n",
    "- **Preprocess observations** (e.g., normalize, crop, stack frames)\n",
    "- **Modify rewards** (e.g., scale, clip, add bonuses)\n",
    "- **Transform actions** (e.g., add noise, discretize continuous actions)\n",
    "- **Add monitoring capabilities** (e.g., record videos, log statistics)\n",
    "- **Implement exploration strategies** (e.g., epsilon-greedy action selection)\n",
    "\n",
    "## Wrapper Class Hierarchy\n",
    "\n",
    "The wrapper system is built around a hierarchy of classes that inherit from the base `Env` class:\n",
    "\n",
    "- **`Wrapper`**: Base wrapper class that can modify any aspect of the environment\n",
    "- **`ObservationWrapper`**: Specialized for modifying observations\n",
    "- **`RewardWrapper`**: Specialized for modifying rewards  \n",
    "- **`ActionWrapper`**: Specialized for modifying actions\n",
    "\n",
    "Each wrapper \"wraps\" an existing environment and can be stacked with other wrappers to create complex preprocessing pipelines.\n",
    "\n",
    "## Types of Wrappers\n",
    "\n",
    "### ObservationWrapper\n",
    "Used when you want to modify the observations returned by the environment. You override the `observation(obs)` method to transform observations before they reach the agent.\n",
    "\n",
    "**Common use cases:**\n",
    "- Normalizing pixel values\n",
    "- Cropping or resizing images\n",
    "- Stacking multiple frames\n",
    "- Adding noise to observations\n",
    "\n",
    "### RewardWrapper  \n",
    "Used to modify the reward signal. You override the `reward(rew)` method to transform rewards.\n",
    "\n",
    "**Common use cases:**\n",
    "- Scaling rewards to a specific range\n",
    "- Clipping extreme reward values\n",
    "- Adding shaped rewards or bonuses\n",
    "- Normalizing rewards across episodes\n",
    "\n",
    "### ActionWrapper\n",
    "Used to modify actions before they're sent to the environment. You override the `action(action)` method.\n",
    "\n",
    "**Common use cases:**\n",
    "- Adding exploration noise\n",
    "- Discretizing continuous action spaces\n",
    "- Implementing action repeat\n",
    "- Adding action constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9933731",
   "metadata": {},
   "source": [
    "## Rendering Wrappers\n",
    "\n",
    "Gymnasium provides specialized wrappers for monitoring and recording environment interactions. These replace the older Monitor wrapper from OpenAI Gym.\n",
    "\n",
    "### HumanRendering Wrapper\n",
    "Opens a graphical window to display the environment in real-time. Useful for:\n",
    "- Debugging agent behavior visually\n",
    "- Creating demonstrations\n",
    "- Real-time monitoring during training\n",
    "\n",
    "### RecordVideo Wrapper  \n",
    "Captures environment frames and creates video files of agent episodes. Useful for:\n",
    "- Creating training videos\n",
    "- Recording best-performing episodes\n",
    "- Documentation and presentations\n",
    "- Remote monitoring (when GUI is not available)\n",
    "\n",
    "**Important**: Both wrappers require the environment to be created with `render_mode=\"rgb_array\"` to access pixel data.\n",
    "\n",
    "\n",
    "## Built-in Wrappers in Gymnasium\n",
    "\n",
    "Gymnasium provides many pre-built wrappers for common use cases:\n",
    "\n",
    "### Observation Wrappers\n",
    "- **`ResizeObservationWrapper`**: Resize image observations\n",
    "- **`GrayScaleObservation`**: Convert RGB images to grayscale\n",
    "- **`FrameStackObservation`**: Stack multiple consecutive frames\n",
    "- **`FlattenObservation`**: Flatten multi-dimensional observations\n",
    "- **`NormalizeObservation`**: Normalize observations with running statistics\n",
    "\n",
    "### Action Wrappers\n",
    "- **`ClipAction`**: Clip continuous actions to valid ranges\n",
    "- **`RescaleAction`**: Rescale actions from one range to another\n",
    "\n",
    "### Reward Wrappers  \n",
    "- **`ClipReward`**: Clip rewards to a specified range\n",
    "- **`NormalizeReward`**: Normalize rewards with running statistics\n",
    "\n",
    "### Utility Wrappers\n",
    "- **`TimeLimit`**: Limit episode length\n",
    "- **`AutoResetWrapper`**: Automatically reset environment when episodes end\n",
    "- **`RecordEpisodeStatistics`**: Track episode statistics (length, reward)\n",
    "\n",
    "### Atari-specific Wrappers\n",
    "- **`AtariPreprocessing`**: Standard preprocessing for Atari games\n",
    "- **`FrameSkipWrapper`**: Skip frames and repeat actions\n",
    "- **`StickyActionWrapper`**: Occasionally repeat previous actions\n",
    "\n",
    "## Key Benefits of Wrappers\n",
    "\n",
    "1. **Modularity**: Each wrapper has a single responsibility\n",
    "2. **Reusability**: Wrappers work with any compatible environment\n",
    "3. **Composability**: Multiple wrappers can be stacked together\n",
    "4. **Standardization**: Common preprocessing patterns are pre-implemented\n",
    "5. **Flexibility**: Easy to create custom wrappers for specific needs\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Order matters**: The sequence of wrapper stacking affects the final behavior\n",
    "- **Keep it simple**: Use existing wrappers when possible before creating custom ones\n",
    "- **Document changes**: Clearly document what each wrapper modifies\n",
    "- **Test thoroughly**: Wrapped environments may behave differently than expected\n",
    "- **Consider performance**: Some wrappers may add computational overhead\n",
    "\n",
    "The wrapper system is one of Gymnasium's most powerful features, enabling complex environment modifications with clean, reusable code. As you progress in reinforcement learning, you'll find wrappers essential for preprocessing observations, shaping rewards, and implementing various training strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd540ce5",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040be3c7",
   "metadata": {},
   "source": [
    "### RandomActionWrapper\n",
    "\n",
    "Let's implement a custom `ActionWrapper` that demonstrates the exploration/exploitation concept. This wrapper will randomly replace the agent's chosen action with a random action with some probability (epsilon):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaef1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing RandomActionWrapper ===\n",
      "Wrapper epsilon (random action probability): 0.2\n",
      "Random action taken: 0 (instead of 0)\n",
      "Episode ended after 10 steps\n",
      "Total reward: 10.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper that replaces agent actions with random actions with probability epsilon.\n",
    "\n",
    "    This is useful for exploration in reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, epsilon: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the wrapper.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to wrap.\n",
    "            epsilon: The probability of taking a random action.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def action(self, action: int) -> int:\n",
    "        \"\"\"Override the action method to potentially replace with random action\n",
    "\n",
    "        Args:\n",
    "            action: The action to take.\n",
    "\n",
    "        Returns:\n",
    "            The action to take.\n",
    "        \"\"\"\n",
    "        # Take a random action with probability epsilon.\n",
    "        if random.random() < self.epsilon:\n",
    "            random_action = self.env.action_space.sample()\n",
    "            print(f\"Random action taken: {random_action} (instead of {action})\")\n",
    "            return random_action\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "# Example usage: Create a deterministic agent that always chooses action 0,\n",
    "# but with random exploration added via the wrapper\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RandomActionWrapper(env, epsilon=0.2)\n",
    "\n",
    "print(\"=== Testing RandomActionWrapper ===\")\n",
    "print(f\"Wrapper epsilon (random action probability): {env.epsilon}\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Run for a few steps with a deterministic policy (always action 0)\n",
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "while total_steps < 20:\n",
    "    # Always choose action 0 (push left), but wrapper may override\n",
    "    obs, reward, terminated, truncated, info = env.step(0)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Episode ended after {total_steps} steps\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1452f8",
   "metadata": {},
   "source": [
    "### NormalizedObservationWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee055b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1268482   0.55425537 -1.5511702  -0.12993342]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class NormalizeObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper that normalizes observations to have zero mean and unit variance.\n",
    "\n",
    "    Useful for neural networks that prefer normalized inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def observation(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize the observation\n",
    "\n",
    "        Args:\n",
    "            obs: The observation to normalize.\n",
    "\n",
    "        Returns:\n",
    "            The normalized observation.\n",
    "        \"\"\"\n",
    "        # Simple normalization: subtract mean and divide by standard deviation.\n",
    "        # In practice, you'd track running statistics and not operate on a single observation.\n",
    "        normalized_obs = (obs - obs.mean()) / (obs.std() + 1e-8)\n",
    "        return normalized_obs\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = NormalizeObservationWrapper(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e83d2a",
   "metadata": {},
   "source": [
    "### RewardScalingWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72ab6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardScalingWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper that scales rewards by a constant factor.\n",
    "\n",
    "    Useful for adjusting reward magnitudes for different algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, scale_factor: float = 0.1):\n",
    "        super().__init__(env)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def reward(self, reward: float) -> float:\n",
    "        \"\"\"Scale the reward\"\"\"\n",
    "        return reward * self.scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d363f7a",
   "metadata": {},
   "source": [
    "### BonusRewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0d5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonusRewardWrapper(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper that adds a bonus reward for staying alive longer.\n",
    "\n",
    "    Encourages the agent to maximize episode length.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, survival_bonus: float = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.survival_bonus = survival_bonus\n",
    "\n",
    "    def reward(self, reward: float) -> float:\n",
    "        \"\"\"Add survival bonus to the original reward.\n",
    "\n",
    "        NOTE: The survival bonus is simply a small additive bonus to the original reward.\n",
    "        \"\"\"\n",
    "        return reward + self.survival_bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107df3c8",
   "metadata": {},
   "source": [
    "### Wrapper stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b59e60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wrapper Stacking Demo ===\n",
      "Wrapper stack (from innermost to outermost):\n",
      "1. CartPole-v1 (base environment)\n",
      "2. NormalizeObservationWrapper\n",
      "3. BonusRewardWrapper\n",
      "4. RewardScalingWrapper\n",
      "\n",
      "Original observation shape: (4,)\n",
      "Normalized observation mean: 0.0000, std: 1.0000\n",
      "Step 1: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 2: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 3: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 4: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 5: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 6: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 7: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 8: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 9: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Step 10: Reward = 0.550 (includes survival bonus + scaling)\n",
      "Total modified reward: 5.500\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate wrapper stacking\n",
    "print(\"=== Wrapper Stacking Demo ===\")\n",
    "\n",
    "# Create base environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Stack multiple wrappers\n",
    "env = NormalizeObservationWrapper(env)  # Normalize observations\n",
    "env = BonusRewardWrapper(env, survival_bonus=0.1)  # Add survival bonus\n",
    "env = RewardScalingWrapper(env, scale_factor=0.5)  # Scale all rewards by 0.5\n",
    "\n",
    "print(\"Wrapper stack (from innermost to outermost):\")\n",
    "print(\"1. CartPole-v1 (base environment)\")\n",
    "print(\"2. NormalizeObservationWrapper\")\n",
    "print(\"3. BonusRewardWrapper\")\n",
    "print(\"4. RewardScalingWrapper\")\n",
    "\n",
    "# Test the wrapped environment\n",
    "obs, info = env.reset()\n",
    "print(f\"\\nOriginal observation shape: {obs.shape}\")\n",
    "print(f\"Normalized observation mean: {obs.mean():.4f}, std: {obs.std():.4f}\")\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Step {step+1}: Reward = {reward:.3f} (includes survival bonus + scaling)\")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Total modified reward: {total_reward:.3f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549881ac",
   "metadata": {},
   "source": [
    "### RecordVideo wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6bb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_record_video():\n",
    "    \"\"\"Demonstrate RecordVideo wrapper\"\"\"\n",
    "    print(\"=== RecordVideo Wrapper Demo ===\")\n",
    "\n",
    "    # Create environment with rgb_array render mode\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "    # Wrap with RecordVideo to save episodes as video files\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder=\"./cartpole_videos\",\n",
    "        episode_trigger=lambda x: True,  # Record every episode\n",
    "    )\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()  # Random actions\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode recorded: {steps} steps, reward: {total_reward}\")\n",
    "    print(\"Video saved to ./cartpole_videos/\")\n",
    "\n",
    "\n",
    "# Run the demos (comment out if you don't want to create files/windows)\n",
    "# demo_record_video()  # Creates video file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
